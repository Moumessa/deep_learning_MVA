{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "nlp_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFmQQhHCxMLN",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for NLP - Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qoMErblxUlx",
        "colab_type": "code",
        "outputId": "4e7d3ca9-28b9-4033-9b4e-ec05dd6b2338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4nOPIAfxVIP",
        "colab_type": "code",
        "outputId": "a96ac83b-2711-4f32-f2cb-b8e31994d8e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd 'drive/My Drive/deep_learning_final_projects/nlp_project'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deep_learning_final_projects/nlp_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZiohSagxMLR",
        "colab_type": "text"
      },
      "source": [
        "RULES:\n",
        "\n",
        "* Do not create any additional cell\n",
        "\n",
        "* Fill in the blanks\n",
        "\n",
        "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
        "\n",
        "* 4 / 20 points will be allocated to the clarity of your code\n",
        "\n",
        "* Efficient code will have a bonus\n",
        "\n",
        "DELIVERABLE:\n",
        "\n",
        "* the pdf with your answers\n",
        "* this notebook\n",
        "* the predictions of the SST test set\n",
        "\n",
        "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "0j0RcOIoxMLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python 3.6 or above is required\n",
        "from collections import defaultdict\n",
        "import gzip\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8BGRA-0xMLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_TO_DATA = Path('data/')\n",
        "# Download word vectors, might take a few minutes and about ~3GB of storage space\n",
        "en_embeddings_path = PATH_TO_DATA / 'cc.en.300.vec.gz'\n",
        "if not en_embeddings_path.exists():\n",
        "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz', en_embeddings_path)\n",
        "fr_embeddings_path = PATH_TO_DATA / 'cc.fr.300.vec.gz'\n",
        "if not fr_embeddings_path.exists():\n",
        "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz', fr_embeddings_path)\n",
        "wiki_news_path = PATH_TO_DATA / 'wiki-news-300d-1M.vec.zip'\n",
        "if not wiki_news_path.exists():\n",
        "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip', wiki_news_path)\n",
        "crawl_300d_2M = PATH_TO_DATA / 'crawl-300d-2M.vec.zip'\n",
        "if not crawl_300d_2M.exists():\n",
        "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', crawl_300d_2M)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kux7MsRzxMLb",
        "colab_type": "text"
      },
      "source": [
        "# 1) Monolingual (English) word embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgwB4dUlxMLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec():\n",
        "\n",
        "    def __init__(self, filepath, vocab_size=50000):\n",
        "        self.words, self.embeddings = self.load_wordvec(filepath, vocab_size)\n",
        "        # Mappings for O(1) retrieval:\n",
        "        self.word2id = {word: idx for idx, word in enumerate(self.words)}\n",
        "        self.id2word = {idx: word for idx, word in enumerate(self.words)}\n",
        "    \n",
        "    def load_wordvec(self, filepath, vocab_size):\n",
        "        assert str(filepath).endswith('.gz') or str(filepath).endswith('.zip')\n",
        "        words = []\n",
        "        embeddings = []\n",
        "        if str(filepath).endswith('.gz') :\n",
        "            with gzip.open(filepath, 'rt') as f:  # Read compressed file directly\n",
        "                next(f)  # Skip header\n",
        "                for i, line in enumerate(f):\n",
        "                    word, vec = line.split(' ', 1)\n",
        "                    words.append(word)\n",
        "                    embeddings.append(np.fromstring(vec, sep=' '))\n",
        "                    if i == (vocab_size - 1):\n",
        "                        break\n",
        "        if str(filepath).endswith('.zip') :\n",
        "            with ZipFile(filepath , 'r') as f:\n",
        "                for name in f.namelist():\n",
        "                    with io.open(name, encoding='utf-8') as ff:\n",
        "                        next(ff)\n",
        "                        for i, line in enumerate(ff):\n",
        "                            word, vec = line.split(' ', 1)\n",
        "                            words.append(word)\n",
        "                            embeddings.append(np.fromstring(vec, sep=' '))\n",
        "                            if i == (vocab_size - 1):\n",
        "                                break\n",
        "\n",
        "        print('Loaded %s pretrained word vectors' % (len(words)))\n",
        "        return words, np.vstack(embeddings)\n",
        "    \n",
        "    def encode(self, word):\n",
        "        # Returns the 1D embedding of a given word\n",
        "        return self.embeddings[self.word2id[word]]\n",
        "    \n",
        "    def score(self, word1, word2):\n",
        "        # Return the cosine similarity: use np.dot & np.linalg.norm\n",
        "        return np.dot(self.encode(word1), self.encode(word2))/(np.linalg.norm(self.encode(word1))*np.linalg.norm(self.encode(word2)))\n",
        "    \n",
        "    def most_similar(self, word, k=5):\n",
        "        # Returns the k most similar words: self.score & np.argsort \n",
        "        scores = []\n",
        "        for w in self.words:\n",
        "          scores.append(self.score(word, w))\n",
        "        sort_ids = np.argsort(np.array(scores))\n",
        "        #return np.array(self.words)[sort_ids[-k:]]\n",
        "        return list( reversed([ self.words[i] for i in sort_ids[-k-1:-1] ] ) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "VbkLB6myxMLg",
        "colab_type": "code",
        "outputId": "86eda320-8089-4f13-e166-545b8777070d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
        "for word1, word2 in zip(('cat', 'cat', 'cat', 'Paris', 'Paris', 'Paris', 'Paris'), ('tree', 'dog', 'pet', 'France', 'Germany', 'baguette', 'donut')):\n",
        "    print(word1, word2, word2vec.score(word1, word2))\n",
        "for word in ['cat', 'dog', 'dogs', 'Paris', 'Germany']:\n",
        "    print(word2vec.most_similar(word))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 50000 pretrained word vectors\n",
            "cat tree 0.2644975466165475\n",
            "cat dog 0.7078641298542562\n",
            "cat pet 0.6753313359976381\n",
            "Paris France 0.6892958925806542\n",
            "Paris Germany 0.4051242286737548\n",
            "Paris baguette 0.29399958277802224\n",
            "Paris donut -0.006588507552348005\n",
            "['cats', 'kitty', 'kitten', 'feline', 'dog']\n",
            "['dogs', 'puppy', 'pup', 'canine', 'pet']\n",
            "['dog', 'cats', 'puppies', 'Dogs', 'pets']\n",
            "['France', 'Parisian', 'Marseille', 'Brussels', 'Strasbourg']\n",
            "['Austria', 'Europe', 'Berlin', 'Hamburg', 'Bavaria']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Cz33zoofxMLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BagOfWords():\n",
        "    \n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "    \n",
        "    def build_idf(self, sentences):\n",
        "        #total number of documents\n",
        "        N=len(sentences)\n",
        "\n",
        "        #tf idf dicts\n",
        "        tf={}\n",
        "        idf={}\n",
        "        #idf={word:0 for word in set(sentences) }\n",
        "\n",
        "        #compute raw count (tf) for each word in every sentence\n",
        "        #for sentence in sentences:\n",
        "          #tf[sentence]= {word: sentence.count(word) for word in set(sentence)}\n",
        "        \n",
        "        #compute \"idf\" for each word\n",
        "        #compute number of sentences where each word appear\n",
        "        for sentence in sentences:\n",
        "          for word in set(sentence.split()):\n",
        "            if word in idf.keys():\n",
        "              idf[word] +=1\n",
        "            else:\n",
        "              idf[word] =1\n",
        "\n",
        "        #compute log N/idfs[word]\n",
        "        idf.update((k, np.log(N/v)) for k,v in idf.items())\n",
        "        return idf\n",
        "    \n",
        "    def encode(self, sentence, idf=None):\n",
        "        # Takes a sentence as input, returns the sentence embedding\n",
        "        if idf is None:\n",
        "            # mean of word vectors\n",
        "            mean_vector = 0\n",
        "            n=0\n",
        "            for word in set(sentence.split()):\n",
        "              if word in self.word2vec.words :\n",
        "                mean_vector += self.word2vec.embeddings[self.word2vec.word2id[word]]\n",
        "                n+=1\n",
        "            mean_vector = mean_vector/n\n",
        "        else:\n",
        "            # idf-weighted mean of word vectors\n",
        "            mean_vector = 0\n",
        "            n=0\n",
        "            for word in set(sentence.split()):\n",
        "              if word in self.word2vec.words and word in idf.keys():\n",
        "                mean_vector += idf[word]*self.word2vec.embeddings[self.word2vec.word2id[word]]\n",
        "                n+=1\n",
        "            mean_vector = mean_vector/n\n",
        "        return mean_vector\n",
        "\n",
        "    def score(self, sentence1, sentence2, idf=None):\n",
        "        # cosine similarity: use np.dot & np.linalg.norm \n",
        "        v1 = self.encode(sentence1, idf=idf)\n",
        "        v2 = self.encode(sentence2, idf=idf)\n",
        "        return np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "    \n",
        "    def most_similar(self, sentence, sentences, idf=None, k=5):\n",
        "        # Return most similar sentences\n",
        "        query = self.encode(sentence, idf)\n",
        "        keys = np.vstack([self.encode(sentence, idf) for sentence in sentences])\n",
        "\n",
        "        l2_query = np.linalg.norm(query)\n",
        "        l2_keys = np.linalg.norm(keys, axis=1)\n",
        "        scores = keys.dot(query)\n",
        "        normalizer = np.multiply(l2_query, l2_keys)\n",
        "        scores = np.divide(scores, normalizer) \n",
        "        arg_idx = np.argsort(scores)[-k-1:-1]\n",
        "        res = [sentences[i] for i in arg_idx]\n",
        "        \n",
        "        return list(reversed(res))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4y_G9zhQxMLn",
        "colab_type": "code",
        "outputId": "7dec6ce7-1258-4a74-ec84-9e6ab093d27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
        "sentence2vec = BagOfWords(word2vec)\n",
        "\n",
        "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
        "filepath = PATH_TO_DATA / 'sentences.txt'\n",
        "with open(filepath, 'r') as f:\n",
        "    sentences = [line.strip('\\n') for line in f]\n",
        "\n",
        "\n",
        "# You will be evaluated on the output of the following:\n",
        "print('\\n\\tAverage of word embeddings')\n",
        "sentence1 = sentences[7]\n",
        "sentence2 = sentences[13]\n",
        "print(sentence1)\n",
        "print(sentence2)\n",
        "print(sentence2vec.score(sentence1, sentence2))\n",
        "sentence = sentences[10]\n",
        "similar_sentences = sentence2vec.most_similar(sentence, sentences)  # BagOfWords-mean\n",
        "print(sentence)\n",
        "for i, sentence in enumerate(similar_sentences):\n",
        "    print(str(i+1) + ')', sentence)\n",
        "\n",
        "# Build idf scores for each word\n",
        "idf = sentence2vec.build_idf(sentences)\n",
        "\n",
        "print('\\n\\tidf weighted average of word embeddings')\n",
        "print(sentence1)\n",
        "print(sentence2)\n",
        "print(sentence2vec.score(sentence1, sentence2, idf))\n",
        "similar_sentences = sentence2vec.most_similar(sentence, sentences, idf)  # BagOfWords-idf\n",
        "print(sentence)\n",
        "for i, sentence in enumerate(similar_sentences):\n",
        "    print(str(i+1) + ')', sentence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 50000 pretrained word vectors\n",
            "\n",
            "\tAverage of word embeddings\n",
            "1 man singing and 1 man playing a saxophone in a concert . \n",
            "10 people venture out to go crosscountry skiing . \n",
            "0.7373762431619013\n",
            "1 smiling african american boy . \n",
            "1) 2 woman dancing while pointing . \n",
            "2) 5 women and 1 man are smiling for the camera . \n",
            "3) a small boy following 4 geese . \n",
            "4) 2 female babies eating chips . \n",
            "5) a young boy and 2 girls open christmas presents . \n",
            "\n",
            "\tidf weighted average of word embeddings\n",
            "1 man singing and 1 man playing a saxophone in a concert . \n",
            "10 people venture out to go crosscountry skiing . \n",
            "0.6465505733624172\n",
            "a young boy and 2 girls open christmas presents . \n",
            "1) 3 girls and one boy playing in the street . \n",
            "2) a group of friends , 3 boys and 2 girls , jump in the air holding hands for a photo . \n",
            "3) five children , 3 boys and 2 girls playing soccer in a grass field . \n",
            "4) 2 young indian girls dressed in colorful dresses walking . \n",
            "5) 2 kids holding hands and smiling . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lHszACnxMLs",
        "colab_type": "text"
      },
      "source": [
        "# 2) Multilingual (English-French) word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP9boSaCxMLt",
        "colab_type": "text"
      },
      "source": [
        "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
        "\n",
        "Let's define **X** and **Y** the **French** and **English** matrices.\n",
        "\n",
        "They contain the embeddings associated to the words in the bilingual dictionary.\n",
        "\n",
        "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
        "\n",
        "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
        "has a closed form solution:\n",
        "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
        "\n",
        "In what follows, you are asked to: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-BvEMU0xMLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultilingualWordAligner:\n",
        "    \n",
        "    def __init__(self, fr_word2vec, en_word2vec):\n",
        "        self.fr_word2vec = fr_word2vec\n",
        "        self.en_word2vec = en_word2vec\n",
        "        self.aligned_fr_embeddings = self.get_aligned_fr_embeddings()\n",
        "        \n",
        "    def get_aligned_fr_embeddings(self):\n",
        "        # 1 - Get words that appear in both vocabs (= identical character strings)\n",
        "        #     Use it to create the matrix X (emb_dim, vocab_size) and Y (emb_dim, vocab_size) (of embeddings for these words)\n",
        "        #raise NotImplementedError('Fill in the blank')\n",
        "        common_words = set(self.fr_word2vec.words).intersection(set(self.en_word2vec.words))\n",
        "        \n",
        "        X=np.empty((self.fr_word2vec.embeddings.shape[1],len(common_words)))\n",
        "        Y=np.empty((self.en_word2vec.embeddings.shape[1],len(common_words)))\n",
        "        \n",
        "        for i, word in enumerate(list(common_words)):\n",
        "\n",
        "          fr_ind = self.fr_word2vec.word2id[word]\n",
        "          en_ind = self.en_word2vec.word2id[word]\n",
        "\n",
        "          X[:,i] = self.fr_word2vec.embeddings[fr_ind]\n",
        "          Y[:,i] = self.en_word2vec.embeddings[en_ind]\n",
        "\n",
        "        assert X.shape[0] == 300 and Y.shape[0] == 300\n",
        "        \n",
        "        # 2 - Solve the Procrustes using the numpy package and: np.linalg.svd() and get the optimal W\n",
        "        #     Now self.fr_word2vec.embeddings * W.transpose() is in the same space as en_word2vec.embeddings\n",
        "        U, S, Vt = np.linalg.svd(np.matmul(Y, X.transpose()))\n",
        "        W=np.matmul(U, Vt)\n",
        "        \n",
        "        assert W.shape == (300, 300)\n",
        "        return np.matmul(fr_word2vec.embeddings, W.transpose())\n",
        "        \n",
        "    def get_closest_english_words(self, fr_word, k=3):\n",
        "        # 3 - Return the top k English nearest neighbors to the input French word\n",
        "        indice = self.fr_word2vec.word2id[fr_word]\n",
        "        query = self.aligned_fr_embeddings[indice]\n",
        "        keys = self.en_word2vec.embeddings\n",
        "\n",
        "        l2_query = np.linalg.norm(query)\n",
        "        l2_keys = np.linalg.norm(keys, axis=1)\n",
        "        scores = keys.dot(query)\n",
        "        normalizer = np.multiply(l2_query, l2_keys)\n",
        "        scores = np.divide(scores, normalizer) \n",
        "        arg_idx = np.argsort(scores)[-k:]\n",
        "\n",
        "        #return np.flip(self.en_word2vec.words[arg_idx],0)\n",
        "        return list( reversed([ self.en_word2vec.words[i] for i in arg_idx[-k:] ] ) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFAHdxG1xMLy",
        "colab_type": "code",
        "outputId": "a04ade38-ef39-48a3-83dd-365e0696f77c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "fr_word2vec = Word2Vec(fr_embeddings_path, vocab_size=50000)\n",
        "en_word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
        "multilingual_word_aligner = MultilingualWordAligner(fr_word2vec, en_word2vec)\n",
        "\n",
        "# You will be evaluated on the output of the following:\n",
        "fr_words = ['chat', 'chien', 'voiture', 'zut']\n",
        "k = 3\n",
        "for fr_word in fr_words:\n",
        "    print('-' * 10)\n",
        "    print(f'fr: \"{fr_word}\"')\n",
        "    en_words = multilingual_word_aligner.get_closest_english_words(fr_word, k=3)\n",
        "    for en_word in en_words:\n",
        "        print(f'en: \"{en_word}\"')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 50000 pretrained word vectors\n",
            "Loaded 50000 pretrained word vectors\n",
            "----------\n",
            "fr: \"chat\"\n",
            "en: \"cat\"\n",
            "en: \"kitten\"\n",
            "en: \"kitty\"\n",
            "----------\n",
            "fr: \"chien\"\n",
            "en: \"dog\"\n",
            "en: \"cat\"\n",
            "en: \"pet\"\n",
            "----------\n",
            "fr: \"voiture\"\n",
            "en: \"car\"\n",
            "en: \"vehicle\"\n",
            "en: \"automobile\"\n",
            "----------\n",
            "fr: \"zut\"\n",
            "en: \"oops\"\n",
            "en: \"Ah\"\n",
            "en: \"ah\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtOc3iidxML2",
        "colab_type": "text"
      },
      "source": [
        "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-3Ty0kixML3",
        "colab_type": "text"
      },
      "source": [
        "# 3) Sentence classification with BoV and scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl_92n-NxML5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
        "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
        "train_filepath = PATH_TO_DATA / 'SST/stsa.fine.train'\n",
        "dev_filepath = PATH_TO_DATA / 'SST/stsa.fine.dev'\n",
        "test_filepath = PATH_TO_DATA / 'SST/stsa.fine.test.X'\n",
        "\n",
        "#let's define a function to load data\n",
        "\n",
        "def load_training_data(file_path):\n",
        "    #This function allows to read the SST training file\n",
        "    with open(file_path,'r') as f:\n",
        "        lines = f.readlines()\n",
        "    y_train= [int(line[0]) for line in lines] \n",
        "    x_train= [line[2:] for line in lines]\n",
        "    return x_train,y_train\n",
        "\n",
        "def load_test_data(file_path):\n",
        "    #This function allows to read the SST test file\n",
        "    with open(file_path,'r') as f:\n",
        "        lines = f.readlines()\n",
        "    x_test = [line for line in lines]\n",
        "    return x_test\n",
        "\n",
        "#now let's load data\n",
        "sst_train_data, sst_train_labels = load_training_data(train_filepath )\n",
        "sst_dev_data, sst_dev_labels = load_training_data(dev_filepath )\n",
        "sst_test = load_test_data(test_filepath )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN82gpQAxML9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 - Encode sentences with the BoV model above\n",
        "\n",
        "#encoding without idf\n",
        "encoded_sst_train_data = np.array([ sentence2vec.encode(sst_train_data[i]) for i in range(len(sst_train_data)) ])\n",
        "encoded_sst_dev_data = np.array([ sentence2vec.encode(sst_dev_data[i]) for i in range(len(sst_dev_data)) ])\n",
        "encoded_sst_test = np.array([ sentence2vec.encode(sst_test[i]) for i in range(len(sst_test)) ])\n",
        "\n",
        "#encoding with idf\n",
        "#idf = sentence2vec.build_idf(sst_train_data)\n",
        "idf = sentence2vec.build_idf(sst_train_data + sst_dev_data+sst_test)\n",
        "idf_encoded_sst_train_data = np.array([ sentence2vec.encode(sst_train_data[i], idf=idf) for i in range(len(sst_train_data)) ])\n",
        "\n",
        "idf_encoded_sst_dev_data = np.array([ sentence2vec.encode(sst_dev_data[i], idf=idf) for i in range(len(sst_dev_data)) ])\n",
        "\n",
        "idf_encoded_sst_test = np.array([ sentence2vec.encode(sst_test[i], idf=idf) for i in range(len(sst_test)) ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNp1DZoQxMMA",
        "colab_type": "code",
        "outputId": "456148d2-1512-4cb9-861c-af6c9b940fa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
        "#     (consider tuning the L2 regularization on the dev set)\n",
        "#     In the paper, the accuracy for average of word vectors is 32.7%\n",
        "#     (VecAvg, table 1, https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
        "\n",
        "accuracies={}\n",
        "# I)-compute prediction for data without idf\n",
        "\n",
        "model_train = LogisticRegression(solver='lbfgs', max_iter=500, multi_class='ovr')\n",
        "model_train.fit(encoded_sst_train_data, sst_train_labels)\n",
        "\n",
        "#case of prediction using dev set\n",
        "train_pred = model_train.predict(encoded_sst_dev_data)\n",
        "accuracies['sst_dev'] =  metrics.accuracy_score(sst_dev_labels, train_pred)\n",
        "\n",
        "#case of prediction using train set\n",
        "train_pred = model_train.predict(encoded_sst_train_data)\n",
        "accuracies['sst_train'] =  metrics.accuracy_score(sst_train_labels, train_pred)\n",
        "\n",
        "# II)-compute prediction for data with idf\n",
        "model_idf_train = LogisticRegression(solver='lbfgs', max_iter=500, multi_class='ovr')\n",
        "model_idf_train.fit(idf_encoded_sst_train_data, sst_train_labels)\n",
        "\n",
        "#case of prediction using dev set\n",
        "train_pred = model_idf_train.predict(idf_encoded_sst_dev_data)\n",
        "accuracies['idf_sst_dev'] =  metrics.accuracy_score(sst_dev_labels, train_pred)\n",
        "\n",
        "#case of prediction using train set\n",
        "train_pred = model_idf_train.predict(idf_encoded_sst_train_data)\n",
        "accuracies['idf_sst_train'] =  metrics.accuracy_score(sst_train_labels, train_pred)\n",
        "\n",
        "#here are accuracies :\n",
        "accuracies"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'idf_sst_dev': 0.40962761126248864,\n",
              " 'idf_sst_train': 0.4693352059925094,\n",
              " 'sst_dev': 0.3905540417801998,\n",
              " 'sst_train': 0.42661516853932585}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY-I-CSoxMME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
        "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
        "#     You will be evaluated on the results of the test set.\n",
        "\n",
        "sst_test_predicted = model_idf_train.predict(encoded_sst_test)\n",
        "np.savetxt('logreg_bov_y_test_sst.txt', sst_test_predicted, fmt='%s') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtaBNfHmxMMH",
        "colab_type": "code",
        "outputId": "3e59ba8a-4447-4ebc-a8dd-3ac48e5288c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# BONUS!\n",
        "# 5 - Try to improve performance with another classifier\n",
        "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
        "\n",
        "# TYPE CODE HERE\n",
        "accuracies2={}\n",
        "# I)-compute prediction for data without idf\n",
        "\n",
        "model_RFC =  RandomForestClassifier(n_estimators=100)\n",
        "model_RFC.fit(encoded_sst_train_data, sst_train_labels)\n",
        "\n",
        "#case of prediction using dev set\n",
        "dev_pred = model_RFC.predict(encoded_sst_dev_data)\n",
        "accuracies2['sst_dev_RFC'] =  metrics.accuracy_score(sst_dev_labels, dev_pred)\n",
        "\n",
        "#case of prediction using train set\n",
        "train_pred = model_RFC.predict(encoded_sst_train_data)\n",
        "accuracies2['sst_train_RFC'] =  metrics.accuracy_score(sst_train_labels, train_pred)\n",
        "\n",
        "# II)-compute prediction for data with idf\n",
        "model_RFC.fit(idf_encoded_sst_train_data, sst_train_labels)\n",
        "\n",
        "#case of prediction using dev set\n",
        "dev_pred = model_RFC.predict(idf_encoded_sst_dev_data)\n",
        "accuracies2['idf_sst_dev_RFC'] =  metrics.accuracy_score(sst_dev_labels, dev_pred)\n",
        "\n",
        "#case of prediction using train set\n",
        "train_pred = model_RFC.predict(idf_encoded_sst_train_data)\n",
        "accuracies2['idf_sst_train_RFC'] =  metrics.accuracy_score(sst_train_labels, train_pred)\n",
        "\n",
        "#here are new accuracies :\n",
        "accuracies2"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'idf_sst_dev_RFC': 0.34877384196185285,\n",
              " 'idf_sst_train_RFC': 0.9985955056179775,\n",
              " 'sst_dev_RFC': 0.3533151680290645,\n",
              " 'sst_train_RFC': 0.9985955056179775}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAE3_H-sJl2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sst_test_predicted_rfc = model_RFC.predict(encoded_sst_test)\n",
        "np.savetxt('RandomForestC_bov_y_test_sst.txt', sst_test_predicted_rfc, fmt='%s') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6BlEbsUxMML",
        "colab_type": "text"
      },
      "source": [
        "# 4) Sentence classification with LSTMs in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkYE_zhhxMMN",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "wYLqSuj-xMMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -U tensorflow\n",
        "#!pip install -U keras\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbZP92uzxMMR",
        "colab_type": "code",
        "outputId": "b9205065-df5c-49f2-f61f-c52471981748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 1 - Using the same dataset, transform text to integers using tf.keras.preprocessing.text.one_hot function\n",
        "#     https://keras.io/preprocessing/text/\n",
        "\n",
        "# TYPE CODE HERE\n",
        "from keras.preprocessing import text\n",
        "\n",
        "n= 50000\n",
        "\n",
        "sst_train_data_one_hot =[text.one_hot(w,n=50000) for w in sst_train_data]\n",
        "sst_dev_data_one_hot =[text.one_hot(w,n=50000) for w in sst_dev_data]\n",
        "sst_test_data_one_hot =[text.one_hot(w,n=50000) for w in sst_test]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGW9EP88xMMV",
        "colab_type": "text"
      },
      "source": [
        "**Padding input data**\n",
        "\n",
        "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
        "\n",
        "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
        "\n",
        "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyckxabkxMMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 - Pad your sequences using tf.keras.preprocessing.sequence.pad_sequences\n",
        "#     https://keras.io/preprocessing/sequence/\n",
        "\n",
        "# TYPE CODE HERE\n",
        "from keras.preprocessing import sequence\n",
        "#tf.keras.preprocessing.sequence\n",
        "\n",
        "sst_train_data_one_hot_padded = sequence.pad_sequences(sst_train_data_one_hot)\n",
        "sst_dev_data_one_hot_padded = sequence.pad_sequences(sst_dev_data_one_hot)\n",
        "sst_test_data_one_hot_padded = sequence.pad_sequences(sst_test_data_one_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dItsEpwTxMMZ",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 - Design and train your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6cqmSXoxMMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3 - Design your encoder + classifier using tensorflow.keras.layers\n",
        "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
        "#     Then we add components to this container : the lookup-table, the LSTM, the classifier etc.\n",
        "#     All of these components are contained in the Sequential() and are trained together.\n",
        "#     Note that the embedding layer is initialized randomly and does not take advantage of pre-trained word embeddings.\n",
        "\n",
        "\n",
        "# ADAPT CODE BELOW\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
        "\n",
        "embed_dim  = 300  # word embedding dimension\n",
        "nhid       = 80  # number of hidden units in the LSTM\n",
        "vocab_size = 50000  # size of the vocabulary\n",
        "n_classes  = 5\n",
        "\n",
        "#model = Sequential()\n",
        "#model.add(Embedding(vocab_size, embed_dim))\n",
        "#model.add(LSTM(nhid, dropout=0.3, recurrent_dropout=0.25))\n",
        "#model.add(Dense(n_classes, activation='sigmoid'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embed_dim))\n",
        "model.add(LSTM(nhid, dropout=0.3, recurrent_dropout=0.25))\n",
        "model.add(Dense(n_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "U-mNSgewxMMh",
        "colab_type": "code",
        "outputId": "d7b11875-df55-4d6d-bdfa-c39e262e0871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# 4 - Define your loss/optimizer/metrics\n",
        "\n",
        "# MODIFY CODE BELOW\n",
        "\n",
        "loss_classif     =  'sparse_categorical_crossentropy' \n",
        "optimizer        =  'rmsprop' #SGD(lr=0.01, momentum=0.9) \n",
        "\n",
        "metrics_classif  =  ['accuracy']\n",
        "\n",
        "# Observe how easy (but blackboxed) this is in Keras\n",
        "model.compile(loss=loss_classif,\n",
        "              optimizer=optimizer,\n",
        "              metrics=metrics_classif)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 300)         15000000  \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 80)                121920    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 405       \n",
            "=================================================================\n",
            "Total params: 15,122,325\n",
            "Trainable params: 15,122,325\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqpKnZQBxMMl",
        "colab_type": "code",
        "outputId": "cb5c872f-e10f-40ea-bac8-6650b013324a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        }
      },
      "source": [
        "# 5 - Train your model and find the best hyperparameters for your dev set\n",
        "#     you will be evaluated on the quality of your predictions on the test set\n",
        "#     Keras expects y_train and y_dev to be one-hot encodings of the labels, i.e. with shape=(n_samples, 5)\n",
        "\n",
        "\n",
        "# ADAPT CODE BELOW\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "mcp1 = ModelCheckpoint('weights.best.hdf5', monitor=\"val_acc\",save_best_only=True, save_weights_only=False)\n",
        "\n",
        "\n",
        "bs = 64\n",
        "n_epochs = 8\n",
        "\n",
        "x_train = sst_train_data_one_hot_padded\n",
        "y_train = np.array(sst_train_labels).astype(int)\n",
        "x_dev = sst_dev_data_one_hot_padded\n",
        "y_dev = np.array(sst_dev_labels).astype(int)\n",
        "\n",
        "#y_train = to_categorical(sst_train_labels)\n",
        "#y_val = to_categorical(sst_dev_labels)\n",
        "\n",
        "want_train =True\n",
        "if want_train:\n",
        "    #history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_dev, y_dev))\n",
        "    history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_dev, y_dev), callbacks = [mcp1])\n",
        "\n",
        "#plotting\n",
        "\n",
        "# plotting accuracy vs epochs\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('LSTM model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# plotting loss vs epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LSTM model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 8544 samples, validate on 1101 samples\n",
            "Epoch 1/8\n",
            "8544/8544 [==============================] - 24s 3ms/sample - loss: 1.5407 - accuracy: 0.3173 - val_loss: 1.4655 - val_accuracy: 0.3660\n",
            "Epoch 2/8\n",
            "  64/8544 [..............................] - ETA: 18s - loss: 1.4007 - accuracy: 0.3438"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8544/8544 [==============================] - 21s 2ms/sample - loss: 1.2967 - accuracy: 0.4478 - val_loss: 1.3651 - val_accuracy: 0.4078\n",
            "Epoch 3/8\n",
            "8544/8544 [==============================] - 19s 2ms/sample - loss: 1.0786 - accuracy: 0.5581 - val_loss: 1.4809 - val_accuracy: 0.3797\n",
            "Epoch 4/8\n",
            "8544/8544 [==============================] - 20s 2ms/sample - loss: 0.8832 - accuracy: 0.6566 - val_loss: 1.5074 - val_accuracy: 0.3833\n",
            "Epoch 5/8\n",
            "8544/8544 [==============================] - 19s 2ms/sample - loss: 0.7154 - accuracy: 0.7342 - val_loss: 1.6761 - val_accuracy: 0.3878\n",
            "Epoch 6/8\n",
            "8544/8544 [==============================] - 19s 2ms/sample - loss: 0.5741 - accuracy: 0.7944 - val_loss: 1.7992 - val_accuracy: 0.3887\n",
            "Epoch 7/8\n",
            "8544/8544 [==============================] - 20s 2ms/sample - loss: 0.4700 - accuracy: 0.8298 - val_loss: 1.9795 - val_accuracy: 0.3651\n",
            "Epoch 8/8\n",
            "8544/8544 [==============================] - 19s 2ms/sample - loss: 0.3815 - accuracy: 0.8633 - val_loss: 2.1903 - val_accuracy: 0.3733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzoLAGGMi3P8P8bXzJD2EV6YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFhDGGGOqZAFhjDGmShYQxhhjqvT/ATqh+lHbpf2LAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlScHjpyxMMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6 - Generate your predictions on the test set using model.predict(x_test)\n",
        "#     https://keras.io/models/model/\n",
        "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
        "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
        "\n",
        "# TYPE CODE HERE\n",
        "sst_test_LSTM_pred = model.predict_classes(sst_test_data_one_hot_padded)\n",
        "np.savetxt('logreg_lstm_y_test_sst.txt', sst_test_LSTM_pred, fmt='%s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WjZ38rHxMMu",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 - innovate !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN0nJlAvq-D2",
        "colab_type": "code",
        "outputId": "4a674115-2958-47a4-da2f-19d0c70ffb5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "# 7 - Open question: find a model that is better on your dev set\n",
        "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
        "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
        "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
        "\n",
        "# TYPE CODE HERE\n",
        "\n",
        "#importation\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing import text\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation, Conv1D, MaxPooling1D, Embedding, Dropout, Flatten, Bidirectional, Reshape, Conv2D, MaxPool2D\n",
        "\n",
        "#prepare data\n",
        "need_to_prepare_data = True\n",
        "\n",
        "if need_to_prepare_data:\n",
        "    unique_word = np.unique(sum([sent.split() for sent in sst_train_data + sst_dev_data + sst_test], []))\n",
        "    num_words = unique_word.shape[0]\n",
        "\n",
        "    tokenizer = text.Tokenizer(num_words = num_words)\n",
        "    tokenizer.fit_on_texts(sst_train_data + sst_dev_data + sst_test)\n",
        "\n",
        "    #X_train = pad_sequences(tokenizer.texts_to_sequences(sst_train_data), maxlen = 50)\n",
        "    #X_val = pad_sequences(tokenizer.texts_to_sequences(sst_dev_data), maxlen = 50)\n",
        "    #X_test = pad_sequences(tokenizer.texts_to_sequences(sst_test), maxlen = 50)\n",
        "\n",
        "    X_train = pad_sequences(tokenizer.texts_to_sequences(sst_train_data))\n",
        "    X_val = pad_sequences(tokenizer.texts_to_sequences(sst_dev_data))\n",
        "    X_test = pad_sequences(tokenizer.texts_to_sequences(sst_test))\n",
        "\n",
        "#create embeddings matrix\n",
        "need_to_build_embed_matrix = True\n",
        "\n",
        "if need_to_build_embed_matrix :\n",
        "    w2v = Word2Vec(wiki_news_path, vocab_size=2000000)\n",
        "    word_idx = tokenizer.word_index\n",
        "    my_num_words = len(word_idx) + 1\n",
        "    embed_mat = np.zeros((my_num_words, 300))\n",
        "\n",
        "    for word, i in word_idx.items():\n",
        "        if word in w2v.words:\n",
        "            embed_mat[i] = w2v.embeddings[w2v.word2id[word]]\n",
        "\n",
        "#define model\n",
        "def Bi_LSTM_model(n_classes = 5):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(my_num_words, 300, weights = [embed_mat]))\n",
        "    #model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(300, return_sequences = True), merge_mode = 'sum'))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(30))\n",
        "    model.add(Dense(n_classes, kernel_regularizer=l2(0.7e-1),activation='softmax'))\n",
        "    \n",
        "    return model\n",
        "#\n",
        "loss_classif     =  'sparse_categorical_crossentropy'\n",
        "optimizer        =  'adam'\n",
        "metrics_classif  =  ['accuracy']\n",
        "\n",
        "#\n",
        "bi_model = Bi_LSTM_model()\n",
        "\n",
        "print(bi_model.summary())\n",
        "\n",
        "bi_model.compile(loss = loss_classif,\n",
        "                  optimizer = optimizer,\n",
        "                  metrics = metrics_classif)\n",
        "\n",
        "need_to_train_bi_lstm_model = True\n",
        "if need_to_train_bi_lstm_model:\n",
        "    bs = 80\n",
        "    n_epochs = 4\n",
        "    \n",
        "    y_train = np.array(sst_train_labels).astype(int)\n",
        "    y_val = np.array(sst_dev_labels).astype(int)\n",
        "\n",
        "    checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', verbose = 1, monitor = 'val_loss',save_best_only = True, mode = 'auto') \n",
        "    \n",
        "    history = bi_model.fit(X_train, y_train, batch_size = bs, epochs = n_epochs, validation_data = (X_val, y_val), callbacks = [checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 999994 pretrained word vectors\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 300)         5352000   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, None, 300)         1442400   \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 128)         192128    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, None, 5)           645       \n",
            "=================================================================\n",
            "Total params: 6,987,173\n",
            "Trainable params: 6,987,173\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 8544 samples, validate on 1101 samples\n",
            "Epoch 1/4\n",
            "8480/8544 [============================>.] - ETA: 0s - loss: 1.7651 - accuracy: 0.3606\n",
            "Epoch 00001: val_loss improved from inf to 1.43683, saving model to model-001.h5\n",
            "8544/8544 [==============================] - 15s 2ms/sample - loss: 1.7629 - accuracy: 0.3606 - val_loss: 1.4368 - val_accuracy: 0.4169\n",
            "Epoch 2/4\n",
            "8480/8544 [============================>.] - ETA: 0s - loss: 1.2357 - accuracy: 0.4876\n",
            "Epoch 00002: val_loss improved from 1.43683 to 1.35545, saving model to model-002.h5\n",
            "8544/8544 [==============================] - 11s 1ms/sample - loss: 1.2348 - accuracy: 0.4886 - val_loss: 1.3555 - val_accuracy: 0.4360\n",
            "Epoch 3/4\n",
            "8480/8544 [============================>.] - ETA: 0s - loss: 0.9394 - accuracy: 0.6164\n",
            "Epoch 00003: val_loss did not improve from 1.35545\n",
            "8544/8544 [==============================] - 9s 1ms/sample - loss: 0.9390 - accuracy: 0.6166 - val_loss: 1.4956 - val_accuracy: 0.4105\n",
            "Epoch 4/4\n",
            "8480/8544 [============================>.] - ETA: 0s - loss: 0.7073 - accuracy: 0.7309\n",
            "Epoch 00004: val_loss did not improve from 1.35545\n",
            "8544/8544 [==============================] - 9s 1ms/sample - loss: 0.7078 - accuracy: 0.7305 - val_loss: 1.8617 - val_accuracy: 0.3951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhHZrksrFreM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prediction on test set:\n",
        "bi_model.load_weights(\"model-002.h5\")\n",
        "sst_test_bidirect_LSTM_pred = bi_model.predict_classes(X_test)\n",
        "np.savetxt('bidirectional_lstm_y_test_sst.txt', sst_test_bidirect_LSTM_pred, fmt='%s')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}